---

- name: ensure airflow user exists
  user:
    name: airflow
    comment: "Airflow user"
    shell: /bin/bash
  tags: airflow

- name: ensure airflow user has authorized_keys set
  authorized_key: user=airflow key="{{ lookup('file', 'public_keys/' + item + '_id_rsa.pub') }}"
  with_items: "{{ ['hadoop', 'airflow'] + airflow_admins }}"
  tags: airflow

- name: add private key for airflow user
  copy:
    dest: /home/airflow/.ssh/id_rsa
    content: "{{ airflow_id_rsa }}"
    mode: 0600
  become: yes
  become_user: airflow
  tags: airflow

- name: add public key for airflow user
  copy:
    src: public_keys/airflow_id_rsa.pub
    dest: /home/airflow/.ssh/id_rsa.pub
    mode: 0644
  become: yes
  become_user: airflow
  tags: airflow

- block:
  - name: assert hdfs /user/airflow dir exists
    shell: /usr/local/hadoop-2.9.2/bin/hdfs dfs -ls /user/airflow
    register: assert_user_airflow_dir_exists
    changed_when: no
    ignore_errors: yes
    tags: airflow

  - name: create hdfs /user/airflow dir if it doesn't exist
    shell: /usr/local/hadoop-2.9.2/bin/hdfs dfs -mkdir -p /user/airflow &&
           /usr/local/hadoop-2.9.2/bin/hdfs dfs -chown airflow:supergroup /user/airflow
    when: assert_user_airflow_dir_exists is failed
    become: yes
    become_user: hadoop
    tags: airflow

  when: airflow_hdfs_support

- name: ensure better bash history for airflow user
  blockinfile:
    dest: /home/airflow/.bashrc
    mode: 0644
    marker: "# {mark} ANSIBLE MANAGED BLOCK airflow-history"
    block: |
      HISTFILESIZE=20000
      HISTSIZE=10000
      HISTTIMEFORMAT='%y-%m-%dT%T  '
  become: yes
  become_user: airflow
  tags: airflow

- name: ensure airflow ~/airflow_home and ~/o3 dirs exists
  file:
    path: /home/airflow/{{ item }}
    state: directory
  with_items:
    - airflow_home
    - airflow_home/dags
    - o3
    - o3/o3
    - o3/o3/dags
    - o3/o3/hooks
    - o3/o3/operators
    - o3/o3/sensors
  become: yes
  become_user: airflow
  tags: airflow

- name: symlink ~/airflow_home/dags/o3 -> ~/o3/o3/dags
  file:
    src: /home/airflow/o3/o3/dags
    dest: /home/airflow/airflow_home/dags/o3
    state: link
    force: yes
  become: yes
  become_user: airflow
  tags: airflow

- name: symlink ~/airflow_home/dags/prod -> ~/o3/prod-dags
  file:
    src: /home/airflow/o3/prod-dags
    dest: /home/airflow/airflow_home/dags/prod
    state: link
    force: yes
  become: yes
  become_user: airflow
  tags: airflow

- name: ensure prod-dags are copied to o3 project root
  copy:
    src: prod-dags/dags/
    dest: /home/airflow/o3/prod-dags
  become: yes
  become_user: airflow
  ignore_errors: yes
  tags: airflow

- name: ensure prod-utils are copied to o3 project root
  copy:
    src: prod-dags/utils/
    dest: /home/airflow/o3/prod-utils
  become: yes
  become_user: airflow
  ignore_errors: yes
  register: copy_over_prod_utils
  tags: airflow

- name: ensure bash scripts in prod-utils are executable
  when: copy_over_prod_utils is changed
  shell: chmod +x /home/airflow/o3/prod-utils/*
  become: yes
  become_user: airflow
  tags: airflow

- name: ensure latest code base is copied to remote server
  copy:
    src: '{{ item }}'
    dest: /home/airflow/o3/{{ item }}
  become: yes
  become_user: airflow
  with_items:
    - o3_cli.py
    - setup.py
    - o3/__init__.py
    - o3/constants.py
    - o3/utils.py
    - o3/dags/__init__.py
    - o3/dags/o3_d_example_dag1.py
    - o3/dags/o3_d_example_dag2.py
    - o3/dags/o3_d_example_dag3.py
    - o3/dags/o3_d_example_dag4.py
    - o3/dags/o3_d_example_dag5.py
    - o3/dags/o3_d_example_dag6.py
    - o3/hooks/__init__.py
    - o3/hooks/hdfs_hook.py
    - o3/hooks/pyhive_hook.py
    - o3/operators/__init__.py
    - o3/operators/convert_log_to_avro_operator.py
    - o3/operators/ensure_dir_operator.py
    - o3/operators/filter_logs_to_percentage_operator.py
    - o3/operators/ingest_avro_into_hive_operator.py
    - o3/operators/move_file_operator.py
    - o3/operators/remove_file_operator.py
    - o3/operators/row_count_operator.py
    - o3/operators/split_log_by_classifiers_operator.py
    - o3/operators/word_count_operator.py
    - o3/sensors/__init__.py
    - o3/sensors/hdfs_sensor.py
    - environment-linux.yml
  tags: airflow

- name: ensure airflow config matches template
  template:
    src: airflow.cfg.j2
    dest: /home/airflow/airflow_home/airflow.cfg
    mode: 0644
  become: yes
  become_user: airflow
  notify:
    - airflow initdb
    - restart airflow-webserver
    - restart airflow-scheduler
  tags: airflow

- name: ensure anaconda3 environment is set up
  include_role:
    name: anaconda3
  vars:
    anaconda3_user: airflow
    anaconda3_environment_name: o3
    anaconda3_environment_file: /home/airflow/o3/environment-linux.yml
  tags: airflow

- name: ensure o3 package is installed within the o3 conda environment
  shell: bash -il -c 'source /home/airflow/anaconda3/bin/activate o3 &&
         pip install -e /home/airflow/o3'
  become: yes
  become_user: airflow
  register: o3_package_installed
  changed_when: "'Found existing installation: o3' not in o3_package_installed.stdout"
  notify:
    - restart airflow-webserver
    - restart airflow-scheduler
  tags: airflow

- name: ensure AIRFLOW_HOME, PATH and JAVA_HOME is set in {{ bash_profile_file }}
  blockinfile:
    path: /home/airflow/{{ bash_profile_file }}
    marker: "# {mark} ANSIBLE MANAGED BLOCK airflow-environment"
    block: |
      export AIRFLOW_GPL_UNIDECODE=yes
      export AIRFLOW_HOME=/home/airflow/airflow_home
      {% if airflow_hdfs_support %}
      export AVRO_TOOLS_PATH=/usr/local/hadoop-2.9.2/share/hadoop/tools/lib/avro-tools-1.7.7.jar
      export PATH=/usr/local/hadoop-2.9.2/bin:/usr/local/hadoop-2.9.2/sbin:$PATH
      export JAVA_HOME={{ java_home }}
      {% endif %}
  become: yes
  become_user: airflow
  tags: airflow

- name: ensure postgres database is started
  docker_container:
    name: airflow_postgres
    image: postgres:9.6
    state: started
    restart_policy: unless-stopped
    volume_driver: local
    volumes:
      - airflow_postgres:/var/lib/postgresql/data
    ports:
      - 5432:5432
    env:
      POSTGRES_DB: airflow
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: "{{ airflow_db_user_password }}"
  tags: airflow

- name: ensure airflow services matches template
  template:
    src: airflow-{{ item }}.service.j2
    dest: /etc/systemd/system/airflow-{{ item }}.service
  with_items:
    - webserver
    - scheduler
  register: add_airflow_services
  notify:
    - restart airflow-webserver
    - restart airflow-scheduler
  tags: airflow

- name: ensure daemon-reload is run on airflow service modification
  shell: systemctl daemon-reload
  when: add_airflow_services is changed
  tags: airflow

- name: ensure traffic is allowed to airflow port {{ airflow_port }} from whitelisted sources
  firewalld:
    rich_rule: 'rule family=ipv4 source address={{ item }} port port={{ airflow_port }} protocol=tcp accept'
    permanent: yes
    immediate: yes
    state: enabled
  with_items: "{{ src_addr_whitelist }}"
  when: ansible_distribution == 'CentOS'
  tags: airflow

- name: ensure airflow services are running and auto-starting
  service:
    name: airflow-{{ item }}
    state: started
    enabled: yes
  with_items:
    - webserver
    - scheduler
  tags: airflow

...
