---

- name: create 4g swapfile
  shell: dd if=/dev/zero of=/swapfile count=4096 bs=1MiB && chmod 0600 /swapfile && mkswap /swapfile
  args:
    creates: /swapfile
  register: create_swapfile
  tags: common

- name: mount /swapfile
  mount:
    src: /swapfile
    path: swap
    fstype: swap
    state: present
  tags: common

- name: enable swap
  shell: swapon -va
  when: create_swapfile is changed
  tags: common

- name: add o3 routing to /etc/hosts
  blockinfile:
    path: /etc/hosts
    insertbefore: BOF
    marker: "# {mark} ANSIBLE MANAGED BLOCK o3-common"
    block: "{{ etc_hosts_routing }}"
    state: "{{ 'present' if etc_hosts_routing else 'absent' }}"
  tags: common

- name: create hadoop user
  user:
    name: hadoop
    comment: "Cluster user"
    shell: /bin/bash
  tags: common

- name: better bash history for 'hadoop' user
  blockinfile:
    dest: /home/hadoop/.bashrc
    owner: hadoop
    group: hadoop
    mode: 0644
    marker: "# {mark} ANSIBLE MANAGED BLOCK o3-common"
    block: |
      HISTFILESIZE=20000
      HISTSIZE=10000
      HISTTIMEFORMAT='%y-%m-%dT%T  '
  tags: common

- name: set up authorized_keys for hadoop user
  authorized_key: user=hadoop key="{{ lookup('file', 'public_keys/' + item + '_id_rsa.pub') }}"
  with_items: "{{ ['hadoop', 'airflow'] + hadoop_admins }}"
  tags: common

- name: add private key for hadoop user
  copy:
    dest: /home/hadoop/.ssh/id_rsa
    content: "{{ hadoop_id_rsa }}"
    owner: hadoop
    group: hadoop
    mode: 0600
  tags: common

- name: add public key for hadoop user
  copy:
    src: public_keys/hadoop_id_rsa.pub
    dest: /home/hadoop/.ssh/id_rsa.pub
    owner: hadoop
    group: hadoop
    mode: 0644
  tags: common

- name: add ssh config for hadoop user
  copy:
    dest: /home/hadoop/.ssh/config
    content: |
      Host 0.0.0.0 o3-*
        StrictHostKeyChecking no
        UserKnownHostsFile=/dev/null
        LogLevel ERROR
    owner: hadoop
    group: hadoop
  tags: common

- name: install jdk 8 (ubuntu)
  apt:
    name: openjdk-8-jdk-headless
    cache_valid_time: 86400
    state: present
  when: ansible_distribution == 'Ubuntu'
  tags: common

- name: install jdk 8 (centos)
  yum:
    name: java-1.8.0-openjdk-devel
    state: present
  when: ansible_distribution == 'CentOS'
  tags: common

- name: ensure lvm vg_sys01 exists
  shell: vgdisplay vg_sys01
  changed_when: false
  ignore_errors: yes
  register: vgdisplay_vg_sys01
  tags: common

- name: configure lvm logical volumes in vg_sys01
  import_tasks: lvm-setup.yml
  when: not (vgdisplay_vg_sys01 is failed)
  tags: common

- name: extract hadoop tarball
  unarchive:
    src: resources/hadoop-2.9.2.tar.gz
    dest: /usr/local
    owner: hadoop
    group: hadoop
    remote_src: no
    creates: /usr/local/hadoop-2.9.2
  register: extract_hadoop_tarball
  tags: common

- name: remove useless .cmd executables in hadoop-2.9.2/sbin and /bin
  file:
    path: /usr/local/hadoop-2.9.2/{{ item }}
    state: absent
  with_items:
    - sbin/hdfs-config.cmd
    - sbin/start-all.cmd
    - sbin/start-dfs.cmd
    - sbin/start-yarn.cmd
    - sbin/stop-all.cmd
    - sbin/stop-dfs.cmd
    - sbin/stop-yarn.cmd
    - bin/hadoop.cmd
    - bin/hdfs.cmd
    - bin/mapred.cmd
    - bin/yarn.cmd
  when: extract_hadoop_tarball is changed
  tags: common

- name: add avro-tools-1.7.7.jar to $HADOOP_HOME/share/hadoop/tools/lib dir
  copy:
    src: resources/avro-tools-1.7.7.jar
    dest: /usr/local/hadoop-2.9.2/share/hadoop/tools/lib/avro-tools-1.7.7.jar
    mode: 0644
    owner: hadoop
    group: hadoop
  tags: common

- name: extract spark tarball
  unarchive:
    src: resources/spark-2.4.0-bin-hadoop2.7.tgz
    dest: /usr/local
    owner: hadoop
    group: hadoop
    remote_src: no
    creates: /usr/local/spark-2.4.0-bin-hadoop2.7
  register: extract_spark_tarball
  tags: common

- name: remove useless .cmd executables in spark-2.4.0-bin-hadoop2.7/bin
  file:
    path: /usr/local/spark-2.4.0-bin-hadoop2.7/{{ item }}
    state: absent
  with_items:
    - bin/beeline.cmd
    - bin/find-spark-home.cmd
    - bin/load-spark-env.cmd
    - bin/pyspark.cmd
    - bin/pyspark2.cmd
    - bin/run-example.cmd
    - bin/spark-class.cmd
    - bin/spark-class2.cmd
    - bin/spark-shell.cmd
    - bin/spark-shell2.cmd
    - bin/spark-sql.cmd
    - bin/spark-sql2.cmd
    - bin/spark-submit.cmd
    - bin/spark-submit2.cmd
    - bin/sparkR.cmd
    - bin/sparkR2.cmd
  when: extract_spark_tarball is changed
  tags: common

- name: ensure log dir for hadoop services exists with correct owner
  file:
    path: /var/log/{{ item }}
    owner: hadoop
    group: hadoop
    state: directory
  with_items:
    - hadoop
    - hadoop/userlogs
  tags: common

- name: configure log dir in hadoop-env.sh
  blockinfile:
    path: /usr/local/hadoop-2.9.2/etc/hadoop/hadoop-env.sh
    insertafter: "^#export HADOOP_LOG_DIR.*"
    marker: "# {mark} ANSIBLE MANAGED BLOCK o3-common"
    block: |
      export HADOOP_LOG_DIR=/var/log/hadoop
    state: present
  tags: common

- name: configure log dir in hadoop-env.sh
  blockinfile:
    path: /usr/local/hadoop-2.9.2/etc/hadoop/yarn-env.sh
    insertafter: "^# default log directory.*"
    marker: "# {mark} ANSIBLE MANAGED BLOCK o3-common"
    block: |
      export YARN_LOG_DIR=/var/log/hadoop
      export YARN_APP_LOGS_DIR=/var/log/hadoop/userlogs
    state: present
  tags: common

- name: add hadoop and spark binaries to path and set JAVA_HOME
  blockinfile:
    path: /home/hadoop/{{ bash_profile_file }}
    marker: "# {mark} ANSIBLE MANAGED BLOCK o3-common"
    block: |
      export HADOOP_HOME=/usr/local/hadoop-2.9.2
      export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
      export HADOOP_LOG_DIR=/var/log/hadoop

      export YARN_LOG_DIR=/var/log/hadoop

      export SPARK_HOME=/usr/local/spark-2.4.0-bin-hadoop2.7
      export SPARK_LOG_DIR=/var/log/hadoop

      export AVRO_TOOLS_PATH=/usr/local/hadoop-2.9.2/share/hadoop/tools/lib/avro-tools-1.7.7.jar

      export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:$LD_LIBRARY_PATH
      export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH
      export JAVA_HOME={{ java_home }}
    state: present
  become: yes
  become_user: hadoop
  tags: common

- name: set JAVA_HOME in /usr/local/hadoop-2.9.2/etc/hadoop/hadoop-env.sh
  lineinfile:
    path: /usr/local/hadoop-2.9.2/etc/hadoop/hadoop-env.sh
    regexp: '^export JAVA_HOME='
    line: export JAVA_HOME={{ java_home }}
  become: yes
  become_user: hadoop
  tags: common

- name: add firewalld port openings
  import_tasks: firewall-update.yml
  when: ansible_distribution == 'CentOS'
  tags: common

...
